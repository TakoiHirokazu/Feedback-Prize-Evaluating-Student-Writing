{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92ee1049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-28 22:09:52.959807: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# library\n",
    "# ========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, KFold,GroupKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import transformers\n",
    "from transformers import LongformerTokenizer, LongformerModel,AutoTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import logging\n",
    "from ast import literal_eval\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "422386ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================\n",
    "# Constant\n",
    "# ==================\n",
    "TRAIN_PATH = \"../data/train.csv\"\n",
    "DATA_DIR = \"../data/longformer-large-4096/\"\n",
    "DATA_PATH = \"../data/train/\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b75c2f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================\n",
    "# Constant\n",
    "# ==================\n",
    "OUTPUT_DIR = f\"../output/team_share\"\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "ex_pred1 = \"019\"\n",
    "ex_pred2 = \"046\"\n",
    "ex_pred3 = \"048\"\n",
    "ex_pred4 = \"051\"\n",
    "ex_pred5 = \"064\"\n",
    "ex_pred6 = \"067\"\n",
    "pred1_path = f\"../output/exp/ex{ex_pred1}\"\n",
    "pred2_path = f\"../output/exp/ex{ex_pred2}\"\n",
    "pred3_path = f\"../output/exp/ex{ex_pred3}\"\n",
    "pred4_path = f\"../output/exp/ex{ex_pred4}\"\n",
    "pred5_path = f\"../output/exp/ex{ex_pred5}\"\n",
    "pred6_path = f\"../output/exp/ex{ex_pred6}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65d8c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============\n",
    "# Configs\n",
    "# ===============\n",
    "SEED = 0\n",
    "N_SPLITS = 5\n",
    "SHUFFLE = True\n",
    "max_len = 2048\n",
    "LABEL_ALL_SUBTOKENS = True\n",
    "MODEL_PATH = 'allenai/longformer-large-4096'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ac8b321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============\n",
    "# Functions\n",
    "# ===============\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acf08079",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, ids, max_len, tokenizer):\n",
    "        self.ids = ids\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # GET TEXT AND WORD LABELS \n",
    "        name = f'{DATA_PATH}{self.ids[index]}.txt'\n",
    "        txt = open(name, 'r').read()\n",
    "        tokens = self.tokenizer.encode_plus(txt, max_length=self.max_len, padding='max_length',\n",
    "                                   truncation=True, return_offsets_mapping=True)\n",
    "        return {\n",
    "          'token': torch.tensor(tokens['input_ids'], dtype=torch.long),\n",
    "          'mask': torch.tensor(tokens['attention_mask'], dtype=torch.long),\n",
    "           }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37ed3fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_map_rev = {0:'Lead', 1:'Position', 2:'Evidence', 3:'Claim', 4:'Concluding Statement',\n",
    "             5:'Counterclaim', 6:'Rebuttal', 7:'blank'}\n",
    "\n",
    "def collate(d,train=True):\n",
    "    mask_len = int(d[\"mask\"].sum(axis=1).max())\n",
    "    if train:\n",
    "        return {\"token\" : d['token'][:,:mask_len],\n",
    "                 \"mask\" : d['mask'][:,:mask_len],\n",
    "                 \"y\" : d['y'][:,:mask_len],\n",
    "                  \"max_len\" : mask_len}\n",
    "    else:\n",
    "        return {\"token\" : d['token'][:,:mask_len],\n",
    "                 \"mask\" : d['mask'][:,:mask_len],\n",
    "                  \"max_len\" : mask_len}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68c9ed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Main\n",
    "# ================================\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "IDS = train.id.unique()\n",
    "id_array = np.array(IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c1aefd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.load(DATA_DIR + f\"targets_{max_len}.npy\")\n",
    "train_tokens = np.load(DATA_DIR + f\"tokens_{max_len}.npy\")\n",
    "train_attention = np.load(DATA_DIR + f\"attention_{max_len}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3156d895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold0:start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 390/390 [00:10<00:00, 37.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold1:start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 390/390 [00:06<00:00, 62.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold2:start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 390/390 [00:06<00:00, 61.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold3:start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 390/390 [00:06<00:00, 61.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold4:start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 390/390 [00:06<00:00, 61.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# train\n",
    "# ================================\n",
    "pred_len = np.ndarray((0))\n",
    "pred_id = np.ndarray((0))\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=SHUFFLE, random_state=SEED)\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(id_array)):\n",
    "    print(f\"fold{fold}:start\")\n",
    "    x_val_id  = id_array[valid_idx]\n",
    "    pred_dataset = TestDataset(x_val_id, max_len, tokenizer)\n",
    "    pred_loader = DataLoader(pred_dataset, \n",
    "                     batch_size=8,\n",
    "                     shuffle=False, \n",
    "                     pin_memory=True, drop_last=False)\n",
    "    pred_len_ = np.ndarray((0))\n",
    "    with torch.no_grad():  \n",
    "        for d in tqdm(pred_loader,total=len(pred_loader)):\n",
    "            d = collate(d,train=False)\n",
    "            ids = d['token']\n",
    "            pred_len_ = np.concatenate([pred_len_,np.array([d[\"max_len\"] for i in range(len(ids))])],axis=0)\n",
    "    pred_len = np.concatenate([pred_len, pred_len_],axis=0)\n",
    "    pred_id = np.concatenate([pred_id, x_val_id],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b1fc4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred1\n",
    "for i in range(5):\n",
    "    if i == 0:\n",
    "        oof_pred1 = np.load(pred1_path + f\"/ex{ex_pred1}_oof_npy_{i}.npy\")\n",
    "        fold = np.zeros(len(oof_pred1))\n",
    "    else:\n",
    "        oof_pred_ = np.load(pred1_path + f\"/ex{ex_pred1}_oof_npy_{i}.npy\")\n",
    "        oof_pred1 = np.concatenate([oof_pred1,oof_pred_],axis=0)\n",
    "        fold_ = np.array([i for _ in range(len(oof_pred_))])\n",
    "        fold = np.concatenate([fold,fold_],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "712e5437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred2\n",
    "\n",
    "for i in range(5):\n",
    "    if i == 0:\n",
    "        oof_pred2 = np.load(pred2_path + f\"/ex{ex_pred2}_oof_npy_{i}.npy\")\n",
    "    else:\n",
    "        oof_pred_ = np.load(pred2_path + f\"/ex{ex_pred2}_oof_npy_{i}.npy\")\n",
    "        oof_pred2 = np.concatenate([oof_pred2,oof_pred_],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b513ad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred3\n",
    "\n",
    "for i in range(5):\n",
    "    if i == 0:\n",
    "        oof_pred3 = np.load(pred3_path + f\"/ex{ex_pred3}_oof_npy_{i}.npy\")\n",
    "    else:\n",
    "        oof_pred_ = np.load(pred3_path + f\"/ex{ex_pred3}_oof_npy_{i}.npy\")\n",
    "        oof_pred3 = np.concatenate([oof_pred3,oof_pred_],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3077dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred4\n",
    "\n",
    "for i in range(5):\n",
    "    if i == 0:\n",
    "        oof_pred4 = np.load(pred4_path + f\"/ex{ex_pred4}_oof_npy_{i}.npy\")\n",
    "    else:\n",
    "        oof_pred_ = np.load(pred4_path + f\"/ex{ex_pred4}_oof_npy_{i}.npy\")\n",
    "        oof_pred4 = np.concatenate([oof_pred4,oof_pred_],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6b7259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred5\n",
    "\n",
    "for i in range(5):\n",
    "    if i == 0:\n",
    "        oof_pred5 = np.load(pred5_path + f\"/ex{ex_pred5}_oof_npy_{i}.npy\")\n",
    "    else:\n",
    "        oof_pred_ = np.load(pred5_path + f\"/ex{ex_pred5}_oof_npy_{i}.npy\")\n",
    "        oof_pred5 = np.concatenate([oof_pred5,oof_pred_],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "896c4b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred6\n",
    "\n",
    "for i in range(5):\n",
    "    if i == 0:\n",
    "        oof_pred6 = np.load(pred6_path + f\"/ex{ex_pred6}_oof_npy_{i}.npy\")\n",
    "    else:\n",
    "        oof_pred_ = np.load(pred6_path + f\"/ex{ex_pred6}_oof_npy_{i}.npy\")\n",
    "        oof_pred6 = np.concatenate([oof_pred6,oof_pred_],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "113f4ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(OUTPUT_DIR + \"/ex019_longformer_large_2048.npy\",oof_pred1)\n",
    "np.save(OUTPUT_DIR + \"/ex046_roberta_large_512.npy\",oof_pred2)\n",
    "np.save(OUTPUT_DIR + \"/ex048_bart_large_512.npy\",oof_pred3)\n",
    "np.save(OUTPUT_DIR + \"/ex051_funnel_large_512.npy\",oof_pred4)\n",
    "np.save(OUTPUT_DIR + \"/ex064_distilbart_cnn_12_6_512.npy\",oof_pred5)\n",
    "np.save(OUTPUT_DIR + \"/ex067_deberta_large_1024.npy\",oof_pred6)\n",
    "np.save(OUTPUT_DIR + \"/pred_len.npy\",pred_len)\n",
    "np.save(OUTPUT_DIR + \"/pred_id.npy\",pred_id)\n",
    "np.save(OUTPUT_DIR + \"/fold.npy\",fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65833db5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
