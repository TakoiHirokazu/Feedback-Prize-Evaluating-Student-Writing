{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ab9155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# library\n",
    "# ========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, KFold,GroupKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import transformers\n",
    "from transformers import LongformerTokenizer, LongformerModel,AutoTokenizer,RobertaModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import logging\n",
    "from ast import literal_eval\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed7226de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================\n",
    "# Constant\n",
    "# ==================\n",
    "TRAIN_PATH = \"../data/train.csv\"\n",
    "DATA_DIR = \"../data/roberta-large/\"\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3802360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============\n",
    "# Configs\n",
    "# ===============\n",
    "max_len = 512\n",
    "MODEL_PATH = 'roberta-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdce8d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============\n",
    "# Functions\n",
    "# ===============\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8b7c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_map_rev = {0:'Lead', 1:'Position', 2:'Evidence', 3:'Claim', 4:'Concluding Statement',\n",
    "             5:'Counterclaim', 6:'Rebuttal', 7:'blank'}\n",
    "\n",
    "def get_preds_collatte(dataset, verbose,text_ids, preds, preds_len):\n",
    "    all_predictions = []\n",
    "\n",
    "    for id_num in tqdm(range(len(preds))):\n",
    "    \n",
    "        # GET ID\n",
    "        #if (id_num%100==0)&(verbose): \n",
    "        #    print(id_num,', ',end='')\n",
    "        n = text_ids[id_num]\n",
    "        max_len = int(preds_len[id_num])\n",
    "        # GET TOKEN POSITIONS IN CHARS\n",
    "        name = f'../data/{dataset}/{n}.txt'\n",
    "        txt = open(name, 'r').read()\n",
    "        tokens = tokenizer.encode_plus(txt, max_length=max_len, padding='max_length',\n",
    "                                   truncation=True, return_offsets_mapping=True)\n",
    "        off = tokens['offset_mapping']\n",
    "    \n",
    "        # GET WORD POSITIONS IN CHARS\n",
    "        w = []\n",
    "        blank = True\n",
    "        for i in range(len(txt)):\n",
    "            if (txt[i]!=' ')&(txt[i]!='\\n')&(txt[i]!='\\xa0')&(txt[i]!='\\x85')&(blank==True):\n",
    "                w.append(i)\n",
    "                blank=False\n",
    "            elif (txt[i]==' ')|(txt[i]=='\\n')|(txt[i]=='\\xa0')|(txt[i]=='\\x85'):\n",
    "                blank=True\n",
    "        w.append(1e6)\n",
    "            \n",
    "        # MAPPING FROM TOKENS TO WORDS\n",
    "        word_map = -1 * np.ones(max_len,dtype='int32')\n",
    "        w_i = 0\n",
    "        for i in range(len(off)):\n",
    "            if off[i][1]==0: continue\n",
    "            while off[i][0]>=w[w_i+1]: w_i += 1\n",
    "            word_map[i] = int(w_i)\n",
    "        \n",
    "        # CONVERT TOKEN PREDICTIONS INTO WORD LABELS\n",
    "        ### KEY: ###\n",
    "        # 0: LEAD_B, 1: LEAD_I\n",
    "        # 2: POSITION_B, 3: POSITION_I\n",
    "        # 4: EVIDENCE_B, 5: EVIDENCE_I\n",
    "        # 6: CLAIM_B, 7: CLAIM_I\n",
    "        # 8: CONCLUSION_B, 9: CONCLUSION_I\n",
    "        # 10: COUNTERCLAIM_B, 11: COUNTERCLAIM_I\n",
    "        # 12: REBUTTAL_B, 13: REBUTTAL_I\n",
    "        # 14: NOTHING i.e. O\n",
    "        ### NOTE THESE VALUES ARE DIVIDED BY 2 IN NEXT CODE LINE\n",
    "        pred = preds[id_num,]/2.0\n",
    "    \n",
    "        i = 0\n",
    "        while i<max_len:\n",
    "            prediction = []\n",
    "            start = pred[i]\n",
    "            if start in [0,1,2,3,4,5,6,7]:\n",
    "                prediction.append(word_map[i])\n",
    "                i += 1\n",
    "                if i>=max_len: break\n",
    "                while pred[i]==start+0.5:\n",
    "                    if not word_map[i] in prediction:\n",
    "                        prediction.append(word_map[i])\n",
    "                    i += 1\n",
    "                    if i>=max_len: break\n",
    "            else:\n",
    "                i += 1\n",
    "            prediction = [x for x in prediction if x!=-1]\n",
    "            if len(prediction)>4:\n",
    "                all_predictions.append( (n, target_map_rev[int(start)], \n",
    "                                ' '.join([str(x) for x in prediction]) ) )\n",
    "                \n",
    "    # MAKE DATAFRAME\n",
    "    df = pd.DataFrame(all_predictions)\n",
    "    df.columns = ['id','class','predictionstring']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def calc_overlap(row):\n",
    "    \"\"\"\n",
    "    Calculates the overlap between prediction and\n",
    "    ground truth and overlap percentages used for determining\n",
    "    true positives.\n",
    "    \"\"\"\n",
    "    set_pred = set(row.predictionstring_pred.split(' '))\n",
    "    set_gt = set(row.predictionstring_gt.split(' '))\n",
    "    # Length of each and intersection\n",
    "    len_gt = len(set_gt)\n",
    "    len_pred = len(set_pred)\n",
    "    inter = len(set_gt.intersection(set_pred))\n",
    "    overlap_1 = inter / len_gt\n",
    "    overlap_2 = inter/ len_pred\n",
    "    return [overlap_1, overlap_2]\n",
    "\n",
    "\n",
    "def score_feedback_comp(pred_df, gt_df):\n",
    "    \"\"\"\n",
    "    A function that scores for the kaggle\n",
    "        Student Writing Competition\n",
    "        \n",
    "    Uses the steps in the evaluation page here:\n",
    "        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n",
    "    \"\"\"\n",
    "    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n",
    "        .reset_index(drop=True).copy()\n",
    "    pred_df = pred_df[['id','class','predictionstring']] \\\n",
    "        .reset_index(drop=True).copy()\n",
    "    pred_df['pred_id'] = pred_df.index\n",
    "    gt_df['gt_id'] = gt_df.index\n",
    "    # Step 1. all ground truths and predictions for a given class are compared.\n",
    "    joined = pred_df.merge(gt_df,\n",
    "                           left_on=['id','class'],\n",
    "                           right_on=['id','discourse_type'],\n",
    "                           how='outer',\n",
    "                           suffixes=('_pred','_gt')\n",
    "                          )\n",
    "    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n",
    "    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n",
    "\n",
    "    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n",
    "\n",
    "    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n",
    "    # and the overlap between the prediction and the ground truth >= 0.5,\n",
    "    # the prediction is a match and considered a true positive.\n",
    "    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n",
    "    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n",
    "    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n",
    "\n",
    "\n",
    "    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n",
    "    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n",
    "    tp_pred_ids = joined.query('potential_TP') \\\n",
    "        .sort_values('max_overlap', ascending=False) \\\n",
    "        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n",
    "\n",
    "    # 3. Any unmatched ground truths are false negatives\n",
    "    # and any unmatched predictions are false positives.\n",
    "    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n",
    "\n",
    "    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n",
    "    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n",
    "\n",
    "    # Get numbers of each type\n",
    "    TP = len(tp_pred_ids)\n",
    "    FP = len(fp_pred_ids)\n",
    "    FN = len(unmatched_gt_ids)\n",
    "    #calc microf1\n",
    "    my_f1_score = TP / (TP + 0.5*(FP+FN))\n",
    "    return my_f1_score\n",
    "\n",
    "def collatte(d,train=True):\n",
    "    mask_len = int(d[\"mask\"].sum(axis=1).max())\n",
    "    if train:\n",
    "        return {\"token\" : d['token'][:,:mask_len],\n",
    "                 \"mask\" : d['mask'][:,:mask_len],\n",
    "                 \"y\" : d['y'][:,:mask_len],\n",
    "                  \"max_len\" : mask_len}\n",
    "    else:\n",
    "        return {\"token\" : d['token'][:,:mask_len],\n",
    "                 \"mask\" : d['mask'][:,:mask_len],\n",
    "                  \"max_len\" : mask_len}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dee26179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Main\n",
    "# ================================\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "IDS = train.id.unique()\n",
    "id_array = np.array(IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11861a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = max_len\n",
    "# # THE TOKENS AND ATTENTION ARRAYS\n",
    "\n",
    "train_tokens = np.zeros((len(IDS),MAX_LEN), dtype='int32')\n",
    "train_attention = np.zeros((len(IDS),MAX_LEN), dtype='int32')\n",
    "\n",
    "# THE 14 CLASSES FOR NER\n",
    "lead_b = np.zeros((len(IDS),MAX_LEN))\n",
    "lead_i = np.zeros((len(IDS),MAX_LEN))\n",
    "\n",
    "position_b = np.zeros((len(IDS),MAX_LEN))\n",
    "position_i = np.zeros((len(IDS),MAX_LEN))\n",
    "\n",
    "evidence_b = np.zeros((len(IDS),MAX_LEN))\n",
    "evidence_i = np.zeros((len(IDS),MAX_LEN))\n",
    "\n",
    "claim_b = np.zeros((len(IDS),MAX_LEN))\n",
    "claim_i = np.zeros((len(IDS),MAX_LEN))\n",
    "\n",
    "conclusion_b = np.zeros((len(IDS),MAX_LEN))\n",
    "conclusion_i = np.zeros((len(IDS),MAX_LEN))\n",
    "\n",
    "counterclaim_b = np.zeros((len(IDS),MAX_LEN))\n",
    "counterclaim_i = np.zeros((len(IDS),MAX_LEN))\n",
    "\n",
    "rebuttal_b = np.zeros((len(IDS),MAX_LEN))\n",
    "rebuttal_i = np.zeros((len(IDS),MAX_LEN))\n",
    "\n",
    "# HELPER VARIABLES\n",
    "train_lens = []\n",
    "targets_b = [lead_b, position_b, evidence_b, claim_b, conclusion_b, counterclaim_b, rebuttal_b]\n",
    "targets_i = [lead_i, position_i, evidence_i, claim_i, conclusion_i, counterclaim_i, rebuttal_i]\n",
    "target_map = {'Lead':0, 'Position':1, 'Evidence':2, 'Claim':3, 'Concluding Statement':4,\n",
    "             'Counterclaim':5, 'Rebuttal':6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ce91b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15594/15594 [04:35<00:00, 56.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# # WE ASSUME DATAFRAME IS ASCENDING WHICH IT IS\n",
    "# assert( np.sum(train.groupby('id')['discourse_start'].diff()<=0)==0 )\n",
    "\n",
    "# # FOR LOOP THROUGH EACH TRAIN TEXT\n",
    "for id_num in tqdm(range(len(IDS))):\n",
    "        \n",
    "    # READ TRAIN TEXT, TOKENIZE, AND SAVE IN TOKEN ARRAYS    \n",
    "    n = IDS[id_num]\n",
    "    name = f'../data/train/{n}.txt'\n",
    "    txt = open(name, 'r').read()\n",
    "    train_lens.append( len(txt.split()))\n",
    "    tokens = tokenizer.encode_plus(txt, max_length=MAX_LEN, padding='max_length',\n",
    "                                   truncation=True, return_offsets_mapping=True)\n",
    "    train_tokens[id_num,] = tokens['input_ids']\n",
    "    train_attention[id_num,] = tokens['attention_mask']\n",
    "    \n",
    "    # FIND TARGETS IN TEXT AND SAVE IN TARGET ARRAYS\n",
    "    offsets = tokens['offset_mapping']\n",
    "    offset_index = 1\n",
    "    df = train.loc[train.id==n]\n",
    "    for index,row in df.iterrows():\n",
    "        a = row.discourse_start\n",
    "        b = row.discourse_end\n",
    "        if offset_index>len(offsets)-1:\n",
    "            break\n",
    "        c = offsets[offset_index][0]\n",
    "        d = offsets[offset_index][1]\n",
    "        beginning = True\n",
    "        while b>c:\n",
    "            if (c>=a)&(b>=d):\n",
    "                k = target_map[row.discourse_type]\n",
    "                if beginning:\n",
    "                    targets_b[k][id_num][offset_index] = 1\n",
    "                    beginning = False\n",
    "                else:\n",
    "                    targets_i[k][id_num][offset_index] = 1\n",
    "            offset_index += 1\n",
    "            if offset_index>len(offsets)-1:\n",
    "                break\n",
    "            c = offsets[offset_index][0]\n",
    "            d = offsets[offset_index][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c1a116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.zeros((len(IDS),MAX_LEN,15), dtype='int32')\n",
    "for k in range(7):\n",
    "    targets[:,:,2*k] = targets_b[k]\n",
    "    targets[:,:,2*k+1] = targets_i[k]\n",
    "targets[:,:,14] = 1-np.max(targets,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1bf01c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_tokens_long = np.load(\"../data/longformer-large-4096/tokens_2048.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9bb50477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in tqdm(range(len(train_tokens))):\n",
    "#     assert sum(train_tokens[i,:511] == train_tokens_long[i,:511]) == 511"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3cbecaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(DATA_DIR + f\"targets_{max_len}.npy\",targets)\n",
    "np.save(DATA_DIR + f\"tokens_{max_len}.npy\",train_tokens)\n",
    "np.save(DATA_DIR + f\"attention_{max_len}.npy\",train_attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
